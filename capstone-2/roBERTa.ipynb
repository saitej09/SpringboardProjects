{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('tweet-sentiment-extraction-data/train.csv')\n",
    "train['text']=train['text'].astype(str)\n",
    "train['selected_text']=train['selected_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('tweet-sentiment-extraction-data/test.csv')\n",
    "test['text']=test['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str(str1).lower().split()) \n",
    "    b = set(str(str2).lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(axis=0, how='any', inplace=True)\n",
    "test.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 165\n",
    "PATH = 'tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "vocab_file=PATH + 'vocab-roberta-base.json',\n",
    "merges_file=PATH + 'merges-roberta-base.txt',\n",
    "lowercase=True,\n",
    "add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    \n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ':\n",
    "        chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "    \n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        \n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "        \n",
    "        toks = []\n",
    "        for i,(a,b) in enumerate(offsets):\n",
    "            sm = np.sum(chars[a:b])\n",
    "            if sm>0: toks.append(i) \n",
    "                \n",
    "        s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "        attention_mask[k,:len(enc.ids)+5] = 1\n",
    "        if len(toks)>0:\n",
    "            start_tokens[k,toks[0]+1] = 1\n",
    "            end_tokens[k,toks[-1]+1] = 1\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(256, 5,padding='same')(x[0])\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(128, 5,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 5,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(5)(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(256, 5, padding='same')(x[0])\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(128, 5, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 5, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(5)(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.15), optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9945 - activation_loss: 0.4088 - activation_1_loss: 0.5857\n",
      "Epoch 00001: val_loss improved from inf to 0.99663, saving model to v4-roberta-0.h5\n",
      "2748/2748 [==============================] - 6008s 2s/step - loss: 0.9945 - activation_loss: 0.4088 - activation_1_loss: 0.5857 - val_loss: 0.9966 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5881\n",
      "Epoch 2/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9925 - activation_loss: 0.4083 - activation_1_loss: 0.5841\n",
      "Epoch 00002: val_loss improved from 0.99663 to 0.99549, saving model to v4-roberta-0.h5\n",
      "2748/2748 [==============================] - 6015s 2s/step - loss: 0.9925 - activation_loss: 0.4083 - activation_1_loss: 0.5841 - val_loss: 0.9955 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5870\n",
      "Epoch 3/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9915 - activation_loss: 0.4082 - activation_1_loss: 0.5833\n",
      "Epoch 00003: val_loss did not improve from 0.99549\n",
      "2748/2748 [==============================] - 5973s 2s/step - loss: 0.9915 - activation_loss: 0.4082 - activation_1_loss: 0.5833 - val_loss: 0.9956 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5871\n",
      "Epoch 4/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9905 - activation_loss: 0.4080 - activation_1_loss: 0.5825\n",
      "Epoch 00004: val_loss did not improve from 0.99549\n",
      "2748/2748 [==============================] - 5958s 2s/step - loss: 0.9905 - activation_loss: 0.4080 - activation_1_loss: 0.5825 - val_loss: 0.9958 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5872\n",
      "Epoch 5/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9894 - activation_loss: 0.4078 - activation_1_loss: 0.5816\n",
      "Epoch 00005: val_loss did not improve from 0.99549\n",
      "2748/2748 [==============================] - 5962s 2s/step - loss: 0.9894 - activation_loss: 0.4078 - activation_1_loss: 0.5816 - val_loss: 0.9960 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5874\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9952 - activation_loss: 0.4088 - activation_1_loss: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 0.99199, saving model to v4-roberta-1.h5\n",
      "2748/2748 [==============================] - 5988s 2s/step - loss: 0.9952 - activation_loss: 0.4088 - activation_1_loss: 0.5864 - val_loss: 0.9920 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5836\n",
      "Epoch 2/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9932 - activation_loss: 0.4084 - activation_1_loss: 0.5849\n",
      "Epoch 00002: val_loss improved from 0.99199 to 0.99174, saving model to v4-roberta-1.h5\n",
      "2748/2748 [==============================] - 5981s 2s/step - loss: 0.9932 - activation_loss: 0.4084 - activation_1_loss: 0.5849 - val_loss: 0.9917 - val_activation_loss: 0.4083 - val_activation_1_loss: 0.5834\n",
      "Epoch 3/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9922 - activation_loss: 0.4082 - activation_1_loss: 0.5840\n",
      "Epoch 00003: val_loss did not improve from 0.99174\n",
      "2748/2748 [==============================] - 5959s 2s/step - loss: 0.9922 - activation_loss: 0.4082 - activation_1_loss: 0.5840 - val_loss: 0.9919 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5835\n",
      "Epoch 4/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9910 - activation_loss: 0.4080 - activation_1_loss: 0.5830\n",
      "Epoch 00004: val_loss did not improve from 0.99174\n",
      "2748/2748 [==============================] - 5954s 2s/step - loss: 0.9910 - activation_loss: 0.4080 - activation_1_loss: 0.5830 - val_loss: 0.9927 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5842\n",
      "Epoch 5/5\n",
      "2748/2748 [==============================] - ETA: 0s - loss: 0.9899 - activation_loss: 0.4078 - activation_1_loss: 0.5821\n",
      "Epoch 00005: val_loss did not improve from 0.99174\n",
      "2748/2748 [==============================] - 5944s 2s/step - loss: 0.9899 - activation_loss: 0.4078 - activation_1_loss: 0.5821 - val_loss: 0.9929 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5844\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9952 - activation_loss: 0.4088 - activation_1_loss: 0.5864\n",
      "Epoch 00001: val_loss improved from inf to 0.99244, saving model to v4-roberta-2.h5\n",
      "2749/2749 [==============================] - 5936s 2s/step - loss: 0.9952 - activation_loss: 0.4088 - activation_1_loss: 0.5864 - val_loss: 0.9924 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5840\n",
      "Epoch 2/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9932 - activation_loss: 0.4084 - activation_1_loss: 0.5849\n",
      "Epoch 00002: val_loss improved from 0.99244 to 0.99221, saving model to v4-roberta-2.h5\n",
      "2749/2749 [==============================] - 5943s 2s/step - loss: 0.9932 - activation_loss: 0.4084 - activation_1_loss: 0.5849 - val_loss: 0.9922 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5838\n",
      "Epoch 3/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9922 - activation_loss: 0.4082 - activation_1_loss: 0.5841\n",
      "Epoch 00003: val_loss did not improve from 0.99221\n",
      "2749/2749 [==============================] - 5951s 2s/step - loss: 0.9922 - activation_loss: 0.4082 - activation_1_loss: 0.5841 - val_loss: 0.9923 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5839\n",
      "Epoch 4/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9910 - activation_loss: 0.4080 - activation_1_loss: 0.5831\n",
      "Epoch 00004: val_loss did not improve from 0.99221\n",
      "2749/2749 [==============================] - 5957s 2s/step - loss: 0.9910 - activation_loss: 0.4080 - activation_1_loss: 0.5831 - val_loss: 0.9929 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5844\n",
      "Epoch 5/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9900 - activation_loss: 0.4078 - activation_1_loss: 0.5822\n",
      "Epoch 00005: val_loss did not improve from 0.99221\n",
      "2749/2749 [==============================] - 5957s 2s/step - loss: 0.9900 - activation_loss: 0.4078 - activation_1_loss: 0.5822 - val_loss: 0.9935 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9954 - activation_loss: 0.4089 - activation_1_loss: 0.5865\n",
      "Epoch 00001: val_loss improved from inf to 0.99406, saving model to v4-roberta-3.h5\n",
      "2749/2749 [==============================] - 5960s 2s/step - loss: 0.9954 - activation_loss: 0.4089 - activation_1_loss: 0.5865 - val_loss: 0.9941 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5855\n",
      "Epoch 2/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9931 - activation_loss: 0.4084 - activation_1_loss: 0.5847\n",
      "Epoch 00002: val_loss improved from 0.99406 to 0.99343, saving model to v4-roberta-3.h5\n",
      "2749/2749 [==============================] - 5968s 2s/step - loss: 0.9931 - activation_loss: 0.4084 - activation_1_loss: 0.5847 - val_loss: 0.9934 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5849\n",
      "Epoch 3/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9920 - activation_loss: 0.4082 - activation_1_loss: 0.5838\n",
      "Epoch 00003: val_loss did not improve from 0.99343\n",
      "2749/2749 [==============================] - 5957s 2s/step - loss: 0.9920 - activation_loss: 0.4082 - activation_1_loss: 0.5838 - val_loss: 0.9935 - val_activation_loss: 0.4085 - val_activation_1_loss: 0.5850\n",
      "Epoch 4/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9909 - activation_loss: 0.4080 - activation_1_loss: 0.5829\n",
      "Epoch 00004: val_loss did not improve from 0.99343\n",
      "2749/2749 [==============================] - 5949s 2s/step - loss: 0.9909 - activation_loss: 0.4080 - activation_1_loss: 0.5829 - val_loss: 0.9941 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5855\n",
      "Epoch 5/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9898 - activation_loss: 0.4078 - activation_1_loss: 0.5820\n",
      "Epoch 00005: val_loss did not improve from 0.99343\n",
      "2749/2749 [==============================] - 5964s 2s/step - loss: 0.9898 - activation_loss: 0.4078 - activation_1_loss: 0.5820 - val_loss: 0.9947 - val_activation_loss: 0.4086 - val_activation_1_loss: 0.5861\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9947 - activation_loss: 0.4088 - activation_1_loss: 0.5859\n",
      "Epoch 00001: val_loss improved from inf to 0.99469, saving model to v4-roberta-4.h5\n",
      "2749/2749 [==============================] - 5989s 2s/step - loss: 0.9947 - activation_loss: 0.4088 - activation_1_loss: 0.5859 - val_loss: 0.9947 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5863\n",
      "Epoch 2/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9929 - activation_loss: 0.4084 - activation_1_loss: 0.5845\n",
      "Epoch 00002: val_loss improved from 0.99469 to 0.99448, saving model to v4-roberta-4.h5\n",
      "2749/2749 [==============================] - 5983s 2s/step - loss: 0.9929 - activation_loss: 0.4084 - activation_1_loss: 0.5845 - val_loss: 0.9945 - val_activation_loss: 0.4084 - val_activation_1_loss: 0.5861\n",
      "Epoch 3/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9939 - activation_loss: 0.4087 - activation_1_loss: 0.5853\n",
      "Epoch 00003: val_loss did not improve from 0.99448\n",
      "2749/2749 [==============================] - 5974s 2s/step - loss: 0.9939 - activation_loss: 0.4087 - activation_1_loss: 0.5853 - val_loss: 1.0002 - val_activation_loss: 0.4106 - val_activation_1_loss: 0.5896\n",
      "Epoch 4/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9988 - activation_loss: 0.4106 - activation_1_loss: 0.5882\n",
      "Epoch 00004: val_loss did not improve from 0.99448\n",
      "2749/2749 [==============================] - 5971s 2s/step - loss: 0.9988 - activation_loss: 0.4106 - activation_1_loss: 0.5882 - val_loss: 0.9997 - val_activation_loss: 0.4106 - val_activation_1_loss: 0.5891\n",
      "Epoch 5/5\n",
      "2749/2749 [==============================] - ETA: 0s - loss: 0.9987 - activation_loss: 0.4106 - activation_1_loss: 0.5881\n",
      "Epoch 00005: val_loss did not improve from 0.99448\n",
      "2749/2749 [==============================] - 5970s 2s/step - loss: 0.9987 - activation_loss: 0.4106 - activation_1_loss: 0.5881 - val_loss: 0.9998 - val_activation_loss: 0.4106 - val_activation_1_loss: 0.5892\n"
     ]
    }
   ],
   "source": [
    "jac = [];VER='v4';\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "    hist = model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=5, batch_size=8, verbose=1, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_test = test.shape[0]\n",
    "input_ids_t = np.ones((c_test,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((c_test,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((c_test,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 268s 2s/step\n",
      "111/111 [==============================] - 270s 2s/step\n",
      "111/111 [==============================] - 268s 2s/step\n",
      "111/111 [==============================] - 268s 2s/step\n",
      "111/111 [==============================] - 270s 2s/step\n"
     ]
    }
   ],
   "source": [
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "for i in range(5):\n",
    "    model = build_model()\n",
    "    model.load_weights('v4-roberta-%i.h5'%i)\n",
    "    \n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n",
    "    preds_start += preds[0]/n_splits\n",
    "    preds_end += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 165)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 165)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 165)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model_5 (TFRobertaMo ((None, 165, 768), ( 124645632   input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 165, 256)     983296      tf_roberta_model_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 165, 256)     983296      tf_roberta_model_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 165, 256)     0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 165, 256)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 165, 128)     163968      leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 165, 128)     163968      leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 165, 128)     0           conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 165, 128)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 165, 64)      41024       leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 165, 64)      41024       leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 165, 64)      0           conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 165, 64)      0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 165, 5)       325         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 165, 5)       325         leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 165, 5)       0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 165, 5)       0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 165, 1)       6           leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 165, 1)       6           leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)            (None, 165)          0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)            (None, 165)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 165)          0           flatten_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 165)          0           flatten_11[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 127,022,870\n",
      "Trainable params: 127,022,870\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>exciting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>i like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>e5f0e6ef4b</td>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>tired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>416863ce47</td>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>6332da480c</td>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>depression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>df1baec676</td>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>469e15c5a8</td>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>cute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                               text sentiment  \\\n",
       "0     f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1     96d74cb729   Shanghai is also really exciting (precisely -...  positive   \n",
       "2     eee518ae67  Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3     01082688c6                                        happy bday!  positive   \n",
       "4     33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...          ...                                                ...       ...   \n",
       "3529  e5f0e6ef4b  its at 3 am, im very tired but i can`t sleep  ...  negative   \n",
       "3530  416863ce47  All alone in this old house again.  Thanks for...  positive   \n",
       "3531  6332da480c   I know what you mean. My little dog is sinkin...  negative   \n",
       "3532  df1baec676  _sutra what is your next youtube video gonna b...  positive   \n",
       "3533  469e15c5a8   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n",
       "\n",
       "       selected_text  \n",
       "0               last  \n",
       "1           exciting  \n",
       "2              shame  \n",
       "3              happy  \n",
       "4             i like  \n",
       "...              ...  \n",
       "3529           tired  \n",
       "3530          thanks  \n",
       "3531   depression...  \n",
       "3532            love  \n",
       "3533            cute  \n",
       "\n",
       "[3534 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
